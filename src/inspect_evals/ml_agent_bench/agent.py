"""This file defines the basic agent class that can be used to implement different agents."""

import copy
import json
import os
import re
import sys
from abc import ABC, abstractmethod

import anthropic

from .args import Args
from .environment import Environment
from .LLM import complete_text, complete_text_fast
from .schema import Action, ActionInfo, EnhancedJSONEncoder

initial_prompt = """You are a helpful research assistant. You have access to the following tools:
{tools_prompt}

Research Problem: {task_description}

Always respond in this format exactly:
{format_prompt}
Observation: 
```
the result of the action
```

"""

format_prompt_dict = {
    "Thought": "What you are currently doing, what actions to perform and why",
    "Action": "the action to take, should be one of the names of the tools",
    "Action Input": "the input to the action as a valid JSON string",
}


class Agent(ABC):
    """Base class for agents."""

    def __init__(self, args: Args, env: Environment):
        self.args = args
        self.valid_format_entires = ["Action", "Action Input"]
        self.log_dir = os.path.join(args.log_dir, "agent_log")

        self.action_infos = env.action_infos
        prompt_tool_names = list(env.action_infos.keys())
        self.all_tool_names = copy.deepcopy(prompt_tool_names)
        actions_remove_from_prompt = [
            "Read File",
            "Write File",
            "Append File",
            "Retrieval from Research Log",
            "Append Summary to Research Log",
            "Python REPL",
            "Edit Script Segment (AI)",
        ]
        actions_remove_from_prompt.extend(args.actions_remove_from_prompt)
        for t in actions_remove_from_prompt:
            # remove tool name but in case of missing tool name, don't crash
            try:
                prompt_tool_names.remove(t)
            except:
                pass
        for t in args.actions_add_to_prompt:
            # remove tool name but in case of missing tool name, don't crash
            try:
                prompt_tool_names.append(t)
            except:
                pass
        self.prompt_tool_names = prompt_tool_names
        self.tools_prompt = self.construct_tools_prompt(
            prompt_tool_names, env.action_infos
        )

        self.initial_prompt = initial_prompt.format(
            tools_prompt=self.tools_prompt,
            tool_names=self.prompt_tool_names,
            task_description=env.research_problem,
            format_prompt="\n".join(
                [f"{k}: {format_prompt_dict[k]}" for k in self.valid_format_entires]
            ),
        )

        self.history_steps = []

        self.initialize_logging()

    @abstractmethod
    async def run(self, env: Environment):
        """Run the agent on the environment."""
        ...

    def initialize_logging(self):
        """Initialize logging folder for the agent."""
        if os.path.exists(self.log_dir):
            print("Log dir {} already exists. Overwriting.".format(self.log_dir))
        else:
            os.makedirs(self.log_dir)

        with open(os.path.join(self.log_dir, "main_log"), "w", 1) as f:
            f.write("Enabled Tools in Prompt:" + str(self.prompt_tool_names) + "\n")
            f.write(
                "================================Start=============================\n"
            )

        print(
            "Agent is up! See progress in {}".format(
                os.path.join(self.log_dir, "main_log")
            )
        )

    def save(self, file_path):
        """Save the agent state to a file."""
        with open(file_path, "w") as f:
            try:
                json.dump(self.__dict__, f, indent=4, cls=EnhancedJSONEncoder)
            except:
                print("save agent state failed")
                pass

    ############# Helper Functions ################

    @staticmethod
    def construct_tool_prompt(tool_name: str, action_info: ActionInfo) -> str:
        """Construct the prompt for a single tool."""
        usage = ",\n            ".join(
            [f'"{k}": [{v}]' for k, v in action_info.usage.items()]
        )

        tools_prompt = (
            f"""{action_info.description}
        Usage:
        ```
        Action: {tool_name}
        Action Input: {{
            {usage}
        }}
        Observation: [{action_info.return_value}]
        ```
            """.strip()
            + "\n\n"
        )
        return tools_prompt

    @classmethod
    def construct_tools_prompt(
        cls, tool_names: list[str], action_infos: dict[str, ActionInfo]
    ) -> str:
        """Construct the prompt for all tools."""
        tools_prompt = ""
        for tool_name in tool_names:
            tools_prompt += f"""- {tool_name}:
        """
            tools_prompt += cls.construct_tool_prompt(
                tool_name, action_infos[tool_name]
            )
        return tools_prompt

    @staticmethod
    def sanitize_json_string(s: str) -> str:
        """Try to sanitize a string to be a valid JSON string."""
        s = s.strip("```json").strip("```").strip()
        s = s.replace("\\", "\\\\")  # Escape backslashes first
        s = s.replace("/", "\\/")  # Escape forward slashes
        s = s.replace("\b", "\\b")  # Escape backspaces
        s = s.replace("\f", "\\f")  # Escape form feeds
        s = s.replace("\r", "\\r")  # Escape carriage returns
        s = s.replace("\t", "\\t")  # Escape horizontal tabs
        # triple quotes are a problem
        return re.sub(
            r'"([^"]*)"',
            lambda m: '"' + m.group(1).replace("\n", "\\n").replace('"', '\\"') + '"',
            s,
        )

    @classmethod
    def parse_action_input(cls, s, action_info):
        """Parse the action input from a string to a dictionary using different methods."""
        try:
            try:
                d = json.loads(s)
            except:
                # try to sanitize the string
                s = cls.sanitize_json_string(s)
                d = json.loads(s)
            if set(d.keys()) != set(action_info.usage.keys()):
                raise Exception("Argument mismatch")
            return d
        except Exception as e:
            try:
                # as a fallback, try to match the string with regex
                return cls.parse_action_input_by_matching(s, action_info)
            except:
                raise e

    @staticmethod
    def parse_action_input_by_matching(s, action_info):
        """Parse the action input from a string to a dictionary using regex."""
        entries = list(action_info.usage.keys())
        index = s.find("{")
        s = s[index + 1 :]
        index = s.rfind("}")
        s = s[:index]
        pattern = ""
        for e in entries:
            pattern += f'"{e}":([\s\S]*),\s*'
        pattern = pattern[:-4]
        result = re.search(pattern, s, re.MULTILINE)

        if result is None:
            raise Exception("Invalid Format")
        result = {e: r.strip().strip('"') for e, r in zip(entries, result.groups())}
        # # in case for write to file directly
        # if "content" in result:
        #     import ast
        #     result["content"] = ast.literal_eval("\"" + result["content"] + "\"")
        return result

    @staticmethod
    def format_action(entries, valid_format_entires):
        """Formats the action in a readable format."""
        return "".join([k + ": " + entries[k] for k in valid_format_entires])

    @staticmethod
    def parse_entries(s, entries):
        """Parse the entries from the string generated by LLM using regex."""
        entries = [e.strip() for e in entries]
        pattern = ""
        for e in entries:
            e = e.replace("[", "\[").replace("]", "\]")
            pattern += f"{e}:([\s\S]*)"
        result = re.search(pattern, s, re.MULTILINE)
        if result is None:
            raise Exception("Invalid: " + s)

        parsed = [r for r in result.groups()]
        return {e: parsed[idx] for idx, e in enumerate(entries)}


class BaselineAgent(Agent):
    async def run(self, env: Environment):
        # A simple baseline that always executes train.py and reports final answer
        await env.execute(Action("Execute Script", {"script_name": "train.py"}))
        await env.execute(Action("Final Answer", "Done!"))


initial_prompt = """You are a helpful research assistant. You have access to the following tools:
{tools_prompt}

Research Problem: {task_description}

You do not know anything about this problem so far. 

Follow these instructions and do not forget them:
- First, come up with a high level plan based on your understanding of the problem and available tools and record it in the Research Plan and Status. You can revise the plan later.
- Research Plan and Status should well organized and succinctly keep track of 1) high level plan (can be revised), 2) what steps have been done and what steps are in progress, 3) short results and conclusions of each step after it has been performed. 
- Research Plan and Status must only include progress that has been made by previous steps. It should not include results not directly confirmed by the previous observation. 
- Performance numbers and estimates can only be confirmed and included in the status by running the code and observing the output.
- You should come up with a good experiment design that addresses the problem, and whenever applicable, define and measure the baseline performance of the relevant system or model before attempting any improvements.
- Follow the plan and try to achieve the goal as straightforwardly as possible.
- Highlight the supporting experiment results and reasoning before drawing any conclusions. 
- Do not try installing any new packages or libraries.
- If you believe you have solved the problem, you can use the Final Answer action to submit your answer. You can only submit once, so double check that you have achieved the goal before submitting.

Always respond in this format exactly:
{format_prompt}
Observation: 
```
the result of the action
```

"""


format_prompt_dict = {
    "Reflection": "What does the observation mean? If there is an error, what caused the error and how to debug?",
    "Research Plan and Status": "The full high level research plan, with current status and confirmed results of each step briefly annotated. It must only include progress that has been made by previous steps. If there is any update, enclose the new update text in double asterisks **like this**. If there is no update, just copy the previous step Research Plan and Status. The high level plan from the previous step should be fully retained, unless it is intentionally revised.",
    "Fact Check": "List all objective statements in the updates to Research Plan and Status one by one and point out whether it is guessed versus directly confirmed by the previous observation directly above. Performance numbers can only be confirmed by running the code and observing the output.",
    "Thought": "What you are currently doing, what actions to perform and why",
    "Action": "the action to take, should be one of the names of the tools",
    "Action Input": "the input to the action as a valid JSON string",
}
